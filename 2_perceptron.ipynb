{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSNt4ALRvrpd",
        "outputId": "7ac029b8-3357-4691-aa78-55ca8fa39705"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at epoch 4\n",
            "Input: [0 0] => Predicted Output: 0\n",
            "Input: [0 1] => Predicted Output: 0\n",
            "Input: [1 0] => Predicted Output: 0\n",
            "Input: [1 1] => Predicted Output: 1\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Perceptron:\n",
        "  def __init__(self, input_size, learning_rate=0.1, epochs=100):\n",
        "    self.weights=np.zeros(input_size)\n",
        "    self.bias=0\n",
        "    self.learning_rate=learning_rate\n",
        "    self.epochs=epochs\n",
        "  def activation(self,x):\n",
        "    return 1 if x>=0 else  0\n",
        "  def predict(self,x):\n",
        "    z=np.dot(self.weights,x)+self.bias\n",
        "    return self.activation(z)\n",
        "  def train(self,X,y):\n",
        "    for epoch in range(self.epochs):\n",
        "      updates=0\n",
        "      for i in range(len(X)):\n",
        "        prediction=self.predict(X[i])\n",
        "        error=y[i]-prediction\n",
        "        if error !=0:\n",
        "          self.weights+=self.learning_rate*error*X[i]\n",
        "          self.bias+=self.learning_rate*error\n",
        "          updates+=1\n",
        "      if updates==0:\n",
        "        print(f\"Converged at epoch {epoch+1}\")\n",
        "        break\n",
        "if __name__==\"__main__\":\n",
        "  X=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "  y=np.array([0,0,0,1])\n",
        "  perceptron=Perceptron(input_size=2)\n",
        "  perceptron.train(X,y)\n",
        "  for i in range(len(X)):\n",
        "    print(f\"Input: {X[i]} => Predicted Output: {perceptron.predict(X[i])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **What is a Perceptron?**\n",
        "\n",
        "A **Perceptron** is one of the simplest types of artificial neural networks. It's a binary classifier, meaning it can classify data into two categories (e.g., 0 or 1). The Perceptron learns by adjusting its weights and bias based on errors it makes during training.\n",
        "\n",
        "The Perceptron uses a linear decision boundary to separate data points into two classes. If the data is linearly separable (i.e., you can draw a straight line to separate the two classes), the Perceptron will eventually learn the correct weights and bias.\n",
        "\n",
        "---\n",
        "\n",
        "### **Code Breakdown**\n",
        "\n",
        "#### **1. Class Definition (`Perceptron`)**\n",
        "\n",
        "```python\n",
        "class Perceptron:\n",
        "```\n",
        "\n",
        "This defines a class called `Perceptron`. A class is like a blueprint for creating objects. In this case, we're creating a blueprint for a Perceptron model.\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. Initialization (`__init__`)**\n",
        "\n",
        "```python\n",
        "def __init__(self, input_size, learning_rate=0.1, epochs=100):\n",
        "    self.weights = np.zeros(input_size)\n",
        "    self.bias = 0\n",
        "    self.learning_rate = learning_rate\n",
        "    self.epochs = epochs\n",
        "```\n",
        "\n",
        "- **`input_size`**: This is the number of features (dimensions) in your input data. For example, if each input has 2 features (like `[0, 0]`), then `input_size = 2`.\n",
        "  \n",
        "- **`self.weights`**: These are the parameters that the Perceptron adjusts during training. Initially, they are set to zeros using `np.zeros(input_size)`.\n",
        "\n",
        "- **`self.bias`**: This is an additional parameter that shifts the decision boundary. It starts at `0`.\n",
        "\n",
        "- **`learning_rate`**: This controls how much the weights and bias are updated during training. A smaller learning rate means slower learning, while a larger learning rate means faster learning but could overshoot the optimal solution.\n",
        "\n",
        "- **`epochs`**: This is the maximum number of times the Perceptron will go through the entire dataset during training.\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. Activation Function**\n",
        "\n",
        "```python\n",
        "def activation(self, x):\n",
        "    return 1 if x >= 0 else 0\n",
        "```\n",
        "\n",
        "- This is the **activation function**, which determines the output of the Perceptron.\n",
        "- If the weighted sum of inputs (`x`) is greater than or equal to `0`, the output is `1`. Otherwise, the output is `0`.\n",
        "\n",
        "This is also called a **step function** because it \"steps\" from `0` to `1` at `x = 0`.\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. Prediction (`predict`)**\n",
        "\n",
        "```python\n",
        "def predict(self, x):\n",
        "    z = np.dot(self.weights, x) + self.bias\n",
        "    return self.activation(z)\n",
        "```\n",
        "\n",
        "- **`np.dot(self.weights, x)`**: This calculates the **weighted sum** of the inputs. Each input feature is multiplied by its corresponding weight, and the results are summed up.\n",
        "  \n",
        "- **`+ self.bias`**: The bias is added to the weighted sum to shift the decision boundary.\n",
        "\n",
        "- **`self.activation(z)`**: The result of the weighted sum plus bias (`z`) is passed through the activation function to produce the final output (`0` or `1`).\n",
        "\n",
        "---\n",
        "\n",
        "#### **5. Training (`train`)**\n",
        "\n",
        "```python\n",
        "def train(self, X, y):\n",
        "    for epoch in range(self.epochs):\n",
        "        updates = 0\n",
        "        for i in range(len(X)):\n",
        "            prediction = self.predict(X[i])\n",
        "            error = y[i] - prediction\n",
        "            if error != 0:\n",
        "                self.weights += self.learning_rate * error * X[i]\n",
        "                self.bias += self.learning_rate * error\n",
        "                updates += 1\n",
        "        if updates == 0:\n",
        "            print(f\"Converged at epoch {epoch + 1}\")\n",
        "            break\n",
        "```\n",
        "\n",
        "- **Outer Loop (`for epoch in range(self.epochs)`)**: The Perceptron goes through the entire dataset multiple times (up to `self.epochs`).\n",
        "\n",
        "- **Inner Loop (`for i in range(len(X))`)**: For each input in the dataset (`X[i]`), the Perceptron makes a prediction.\n",
        "\n",
        "- **Prediction (`prediction = self.predict(X[i])`)**: The Perceptron predicts the output for the current input.\n",
        "\n",
        "- **Error Calculation (`error = y[i] - prediction`)**: The error is the difference between the true label (`y[i]`) and the predicted output (`prediction`). If the prediction is correct, the error is `0`.\n",
        "\n",
        "- **Weight and Bias Update**:\n",
        "  - If there's an error (`error != 0`), the weights and bias are updated:\n",
        "    - **`self.weights += self.learning_rate * error * X[i]`**: The weights are adjusted based on the error and the input features.\n",
        "    - **`self.bias += self.learning_rate * error`**: The bias is adjusted based on the error.\n",
        "\n",
        "- **Early Stopping (`if updates == 0`)**: If no updates were made during an epoch (i.e., all predictions were correct), the Perceptron has converged, and training stops early.\n",
        "\n",
        "---\n",
        "\n",
        "#### **6. Example Usage**\n",
        "\n",
        "```python\n",
        "if __name__ == \"__main__\":\n",
        "    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "    y = np.array([0, 0, 0, 1])\n",
        "    perceptron = Perceptron(input_size=2)\n",
        "    perceptron.train(X, y)\n",
        "    for i in range(len(X)):\n",
        "        print(f\"Input: {X[i]} => Predicted Output: {perceptron.predict(X[i])}\")\n",
        "```\n",
        "\n",
        "- **`X`**: This is the input data. Each row represents a sample, and each column represents a feature. For example, `[0, 0]` is one sample with two features.\n",
        "\n",
        "- **`y`**: These are the true labels for the inputs. For example, the label for `[0, 0]` is `0`.\n",
        "\n",
        "- **Training (`perceptron.train(X, y)`)**: The Perceptron is trained on the dataset `X` with labels `y`.\n",
        "\n",
        "- **Testing**: After training, the Perceptron is tested on the same dataset to see if it learned correctly. For each input, it prints the predicted output.\n",
        "\n",
        "---\n",
        "\n",
        "### **How Does the Perceptron Learn?**\n",
        "\n",
        "1. **Initial State**: At the start, the weights and bias are all `0`. The Perceptron makes random predictions because it hasn't learned anything yet.\n",
        "\n",
        "2. **Making Predictions**: For each input, the Perceptron calculates the weighted sum of the inputs, adds the bias, and passes the result through the activation function to get a prediction (`0` or `1`).\n",
        "\n",
        "3. **Calculating Errors**: The Perceptron compares its prediction to the true label. If the prediction is wrong, it calculates the error.\n",
        "\n",
        "4. **Updating Weights and Bias**: If there's an error, the Perceptron adjusts its weights and bias to reduce the error. This is done using the formula:\n",
        "   - `weights += learning_rate * error * input`\n",
        "   - `bias += learning_rate * error`\n",
        "\n",
        "5. **Repeating**: The Perceptron repeats this process for multiple epochs until it converges (i.e., it makes no more errors on the training data).\n",
        "\n",
        "---\n",
        "\n",
        "### **Example Walkthrough**\n",
        "\n",
        "Let's say we're training the Perceptron on the AND gate:\n",
        "\n",
        "- **Inputs (`X`)**: `[[0, 0], [0, 1], [1, 0], [1, 1]]`\n",
        "- **Labels (`y`)**: `[0, 0, 0, 1]`\n",
        "\n",
        "Initially, the weights are `[0, 0]` and the bias is `0`.\n",
        "\n",
        "1. **First Input (`[0, 0]`)**:\n",
        "   - Weighted sum: `0 * 0 + 0 * 0 + 0 = 0`\n",
        "   - Activation: `0` (correct, no update needed)\n",
        "\n",
        "2. **Second Input (`[0, 1]`)**:\n",
        "   - Weighted sum: `0 * 0 + 0 * 1 + 0 = 0`\n",
        "   - Activation: `0` (correct, no update needed)\n",
        "\n",
        "3. **Third Input (`[1, 0]`)**:\n",
        "   - Weighted sum: `0 * 1 + 0 * 0 + 0 = 0`\n",
        "   - Activation: `0` (correct, no update needed)\n",
        "\n",
        "4. **Fourth Input (`[1, 1]`)**:\n",
        "   - Weighted sum: `0 * 1 + 0 * 1 + 0 = 0`\n",
        "   - Activation: `0` (wrong, should be `1`)\n",
        "   - Error: `1 - 0 = 1`\n",
        "   - Update weights: `[0, 0] + 0.1 * 1 * [1, 1] = [0.1, 0.1]`\n",
        "   - Update bias: `0 + 0.1 * 1 = 0.1`\n",
        "\n",
        "After several iterations, the Perceptron will adjust its weights and bias until it correctly predicts all outputs.\n",
        "\n",
        "---\n",
        "\n",
        "The Perceptron is a simple but powerful algorithm for binary classification. It works well when the data is linearly separable. However, if the data is not linearly separable, the Perceptron may not converge, and you would need more advanced models like multi-layer neural networks.\n"
      ],
      "metadata": {
        "id": "qeyzTHuK4IZd"
      }
    }
  ]
}